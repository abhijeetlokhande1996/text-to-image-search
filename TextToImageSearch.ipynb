{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a4bc39-2fcc-43ea-9a22-59ace2de1c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset from kaggle datasets download -d adityajn105/flickr8k and Extract Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8602d82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "import torch.utils.data.dataloader\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "015f1c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CHECKPOINT = \"distilbert/distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d45a470",
   "metadata": {},
   "source": [
    "#### Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9149185c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import albumentations as A\n",
    "import cv2\n",
    "from typing import List, Any\n",
    "\n",
    "class TextToImageSearchDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_filenames: List[str], captions: List[str]) -> None:\n",
    "        super().__init__()\n",
    "        self.image_filenames = image_filenames\n",
    "        self.captions = captions\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "        self.encoded_captions = self.tokenizer(\n",
    "            self.captions, return_tensors=\"pt\", truncation=True, is_split_into_words=False, max_length=30, padding=True)\n",
    "        self.image_transforms = A.Compose([\n",
    "            A.Resize(224, 224, always_apply=True),\n",
    "            A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = {key: torch.tensor(value[index]).clone().detach()\n",
    "                for key, value in self.encoded_captions.items()}\n",
    "        image_filepath = \"./archive/Images/\" + self.image_filenames[index]\n",
    "        image = cv2.imread(image_filepath)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = self.image_transforms(image=image)[\"image\"]\n",
    "        # pytorch expects channels, height, width\n",
    "        item[\"image\"] = torch.tensor(image).permute(2, 0, 1).float()\n",
    "        item[\"caption\"] = self.captions[index]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "    \n",
    "\n",
    "\n",
    "def split_dataset(image_filenames, captions, train_size=0.8, val_size=0.1, test_size=0.1):\n",
    "    assert train_size + val_size + \\\n",
    "        test_size == 1.0, \"Train, validation, and test sizes must sum to 1.0\"\n",
    "\n",
    "    # First, split into train and temp (validation + test)\n",
    "    train_imgs, temp_imgs, train_caps, temp_caps = train_test_split(\n",
    "        image_filenames, captions, train_size=train_size, random_state=42)\n",
    "\n",
    "    # Then split the temp into validation and test\n",
    "    val_imgs, test_imgs, val_caps, test_caps = train_test_split(\n",
    "        temp_imgs, temp_caps, test_size=test_size / (test_size + val_size), random_state=42)\n",
    "\n",
    "    return (train_imgs, train_caps), (val_imgs, val_caps), (test_imgs, test_caps)\n",
    "\n",
    "\n",
    "def get_text_to_image_search_dataset():\n",
    "    df_limited = pd.read_csv(\"./archive/dataset_limited.csv\")\n",
    "    image_filenames: List[str] = df_limited[\"image\"].tolist()\n",
    "    captions: List[str] = df_limited[\"caption\"].tolist()\n",
    "\n",
    "    # return TextToImageSearchDataset(image_filenames, captions)\n",
    "    return image_filenames, captions\n",
    "\n",
    "\n",
    "def get_datasets():\n",
    "    image_filenames, captions = get_text_to_image_search_dataset()\n",
    "\n",
    "    (train_imgs, train_caps), (val_imgs, val_caps), (test_imgs,\n",
    "                                                     test_caps) = split_dataset(image_filenames, captions)\n",
    "\n",
    "    train_dataset = TextToImageSearchDataset(\n",
    "        train_imgs, train_caps)\n",
    "    val_dataset = TextToImageSearchDataset(\n",
    "        val_imgs, val_caps)\n",
    "    test_dataset = TextToImageSearchDataset(\n",
    "        test_imgs, test_caps)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "\n",
    "# train_dataset, val_dataset, test_dataset = get_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9474a295",
   "metadata": {},
   "source": [
    "#### Model Defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5da1bf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device_map() -> str:\n",
    "    if torch.cuda.is_available():\n",
    "        return 'cuda'\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return 'mps'\n",
    "    else:\n",
    "        return 'cpu'\n",
    "\n",
    "\n",
    "DEVICE = get_device_map()\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, trainable: bool = True):\n",
    "        super().__init__()\n",
    "        self.image_encoder = timm.create_model(\n",
    "            model_name=\"resnet50\", pretrained=True, num_classes=0, global_pool=\"avg\")\n",
    "        for param in self.image_encoder.parameters():\n",
    "            param.requires_grad = trainable\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.image_encoder(X)\n",
    "\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, trainable: bool = True, device: str = DEVICE) -> None:\n",
    "        super().__init__()\n",
    "        self.model_checkpoint = MODEL_CHECKPOINT\n",
    "        self.device = device\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained(\n",
    "            self.model_checkpoint)\n",
    "        self.text_encoder = DistilBertModel.from_pretrained(\n",
    "            self.model_checkpoint)\n",
    "        for param in self.text_encoder.parameters():\n",
    "            param.requires_grad = trainable\n",
    "\n",
    "    def forward(self, X, return_last_hidden_state=True):\n",
    "        # tokenised_input = self.tokenizer(X, return_tensors=\"pt\", truncation=True, is_split_into_words=False, max_length=30, padding=True)\n",
    "        output = self.text_encoder(**X)\n",
    "        if not return_last_hidden_state:\n",
    "            return output\n",
    "        else:\n",
    "            return output.last_hidden_state[:, 0, :]\n",
    "\n",
    "\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, input_dim: int, projection_dim: int) -> None:\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(input_dim, projection_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc = nn.Linear(projection_dim, projection_dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.layer_norm = nn.LayerNorm(projection_dim)\n",
    "\n",
    "    def forward(self, X):\n",
    "        projected = self.projection(X)\n",
    "        X = self.gelu(projected)\n",
    "        X = self.fc(X)\n",
    "        X = self.dropout(X)\n",
    "        X = X + projected\n",
    "        X = self.layer_norm(X)\n",
    "        return X\n",
    "\n",
    "\n",
    "class CLIPModel(nn.Module):\n",
    "    def __init__(self, projection_dim: int = 512, temperature: float = 1.0) -> None:\n",
    "        super().__init__()\n",
    "        self.image_encoder = ImageEncoder(trainable=True)\n",
    "        self.text_encoder = TextEncoder(trainable=True)\n",
    "        self.projection_dim = projection_dim\n",
    "\n",
    "        self.image_projection = ProjectionHead(\n",
    "            input_dim=2048, projection_dim=self.projection_dim)\n",
    "        self.text_projection = ProjectionHead(\n",
    "            input_dim=768, projection_dim=self.projection_dim)\n",
    "\n",
    "        self.temperature = temperature\n",
    "\n",
    "    @staticmethod\n",
    "    def cross_entropy(preds, targets, reduction=\"none\"):\n",
    "        log_softmax = nn.LogSoftmax(dim=-1)\n",
    "        loss = (-targets * log_softmax(preds)).sum(1)\n",
    "        if reduction == \"none\":\n",
    "            return loss\n",
    "        elif reduction == \"mean\":\n",
    "            return loss.mean()\n",
    "\n",
    "    def forward(self, X):\n",
    "        image_embeddings = self.image_encoder(X[\"image\"])\n",
    "        text_embeddings = self.text_encoder(\n",
    "            {\"input_ids\": X[\"input_ids\"], \"attention_mask\": X[\"attention_mask\"]})\n",
    "\n",
    "        image_embeddings = self.image_projection(image_embeddings)\n",
    "        text_embeddings = self.text_projection(text_embeddings)\n",
    "\n",
    "        # calculating the Loss using image and text embeddings\n",
    "        logits = (text_embeddings @ image_embeddings.T) / self.temperature\n",
    "\n",
    "        image_similarity = image_embeddings @ image_embeddings.T\n",
    "        text_similarity = text_embeddings @ text_embeddings.T\n",
    "\n",
    "        targets = F.softmax((image_similarity + text_similarity) /\n",
    "                            (2.0 * self.temperature), dim=-1)\n",
    "\n",
    "        text_loss = self.cross_entropy(\n",
    "            preds=logits, targets=targets, reduction=\"none\")\n",
    "\n",
    "        image_loss = self.cross_entropy(\n",
    "            preds=logits.T, targets=targets.T, reduction=\"none\")\n",
    "\n",
    "        loss = (text_loss + image_loss) / 2.0\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6d45c3",
   "metadata": {},
   "source": [
    "#### Compute Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22cb3c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_validation_loss(model, validation_dataloader) -> float:\n",
    "    running_loss = 0.0\n",
    "    count = 0\n",
    "    for batch in tqdm(validation_dataloader, total=len(validation_dataloader)):\n",
    "        caption = batch.pop(\"caption\")\n",
    "        batch = {key: value.to(DEVICE) for key, value in batch.items()}\n",
    "        valid_loss = model(batch)\n",
    "        running_loss += valid_loss\n",
    "        count += 1\n",
    "        break\n",
    "\n",
    "    return running_loss / count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c27495",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b70e43ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/405 [00:00<?, ?it/s]/var/folders/vn/6bflx2h52yj49_wzhyq_90x40000gn/T/ipykernel_15484/1160739717.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(value[index]).clone().detach()\n",
      "  0%|          | 0/405 [00:01<?, ?it/s]\n",
      "  0%|          | 0/51 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.0606527163658613, Valid Loss: 52.20391845703125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import torch.nn.functional as F\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = get_datasets()\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "validation_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "\n",
    "model = CLIPModel()\n",
    "model.to(DEVICE)\n",
    "params = [\n",
    "    {\"params\": model.image_encoder.parameters(), \"lr\": 1e-4},\n",
    "    {\"params\": model.text_encoder.parameters(), \"lr\": 1e-5},\n",
    "    {\"params\": itertools.chain(model.image_projection.parameters(\n",
    "    ), model.text_projection.parameters()), \"lr\": 1e-3, \"weight_decay\": 1e-3},\n",
    "\n",
    "]\n",
    "optimizer = torch.optim.AdamW(params=params, weight_decay=0.0)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer=optimizer, mode=\"min\", patience=1, factor=0.8\n",
    ")\n",
    "train_tqdm_object = tqdm(train_dataloader, total=len(train_dataloader))\n",
    "for epoch in range(1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch in train_tqdm_object:\n",
    "        caption = batch.pop(\"caption\")\n",
    "        batch = {key: value.to(DEVICE) for key, value in batch.items()}\n",
    "        loss = model(batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        break\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss_mean = compute_validation_loss(\n",
    "            model, validation_dataloader)\n",
    "    lr_scheduler.step(valid_loss_mean)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataloader)\n",
    "\n",
    "    print(f\"Epoch: {epoch}, Loss: {\n",
    "            epoch_loss}, Valid Loss: {valid_loss_mean}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee737972",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21361b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/51 [00:00<?, ?it/s]/var/folders/vn/6bflx2h52yj49_wzhyq_90x40000gn/T/ipykernel_15484/1160739717.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(value[index]).clone().detach()\n",
      "100%|██████████| 51/51 [00:11<00:00,  4.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1619, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_image_embeddings(dataloader, model):\n",
    "    # model = CLIPModel().to(DEVICE)\n",
    "    # model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "    model.eval()\n",
    "\n",
    "    valid_image_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            image_features = model.image_encoder(batch[\"image\"].to(DEVICE))\n",
    "            image_embeddings = model.image_projection(image_features)\n",
    "            valid_image_embeddings.append(image_embeddings)\n",
    "    return torch.cat(valid_image_embeddings)\n",
    "\n",
    "\n",
    "image_embeddings = get_image_embeddings(test_dataloader, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21d06e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def find_matches(model, image_embeddings, query, image_filenames, n=9):\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "    encoded_query = tokenizer([query])\n",
    "    batch = {\n",
    "        key: torch.tensor(values).to(DEVICE)\n",
    "        for key, values in encoded_query.items()\n",
    "    }\n",
    "    with torch.no_grad():\n",
    "        text_features = model.text_encoder(batch)\n",
    "        text_embeddings = model.text_projection(text_features)\n",
    "\n",
    "    image_embeddings_n = F.normalize(image_embeddings, p=2, dim=-1)\n",
    "    text_embeddings_n = F.normalize(text_embeddings, p=2, dim=-1)\n",
    "    dot_similarity = text_embeddings_n @ image_embeddings_n.T\n",
    "\n",
    "    _, indices = torch.topk(dot_similarity.squeeze(0), n * 5)\n",
    "    matches = [image_filenames[idx] for idx in indices[::5]]\n",
    "\n",
    "    _, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
    "    for match, ax in zip(matches, axes.flatten()):\n",
    "        image_filepath = \"./archive/Images/\"\n",
    "        image = cv2.imread(f\"{image_filepath}/{match}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        ax.imshow(image)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5773bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_matches(model,\n",
    "             image_embeddings,\n",
    "             query=\"a group of people dancing in a party\",\n",
    "             image_filenames=test_dataset.image_filenames,\n",
    "             n=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17146cb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
